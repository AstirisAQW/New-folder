{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4669eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Data Verification\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Define paths\n",
    "TRAIN_DIR = 'FER-2013/train'\n",
    "TEST_DIR = 'FER-2013/test'\n",
    "\n",
    "# Define emotion classes\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Verify directory structure and count images\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify the dataset structure and count images per class\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET VERIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    train_counts = {}\n",
    "    test_counts = {}\n",
    "    \n",
    "    # Check train directory\n",
    "    print(\"\\nüìÅ TRAINING DATA:\")\n",
    "    print(\"-\" * 60)\n",
    "    for emotion in EMOTIONS:\n",
    "        train_path = os.path.join(TRAIN_DIR, emotion)\n",
    "        if os.path.exists(train_path):\n",
    "            count = len([f for f in os.listdir(train_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "            train_counts[emotion] = count\n",
    "            print(f\"  {emotion:10s}: {count:5d} images\")\n",
    "        else:\n",
    "            print(f\"  {emotion:10s}: MISSING FOLDER ‚ö†Ô∏è\")\n",
    "            train_counts[emotion] = 0\n",
    "    \n",
    "    print(f\"\\n  {'TOTAL':10s}: {sum(train_counts.values()):5d} images\")\n",
    "    \n",
    "    # Check test directory\n",
    "    print(\"\\nüìÅ TEST DATA:\")\n",
    "    print(\"-\" * 60)\n",
    "    for emotion in EMOTIONS:\n",
    "        test_path = os.path.join(TEST_DIR, emotion)\n",
    "        if os.path.exists(test_path):\n",
    "            count = len([f for f in os.listdir(test_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "            test_counts[emotion] = count\n",
    "            print(f\"  {emotion:10s}: {count:5d} images\")\n",
    "        else:\n",
    "            print(f\"  {emotion:10s}: MISSING FOLDER ‚ö†Ô∏è\")\n",
    "            test_counts[emotion] = 0\n",
    "    \n",
    "    print(f\"\\n  {'TOTAL':10s}: {sum(test_counts.values()):5d} images\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return train_counts, test_counts\n",
    "\n",
    "# Display sample images from each class\n",
    "def display_samples(data_dir, emotions, samples_per_class=3):\n",
    "    \"\"\"Display sample images from each emotion class\"\"\"\n",
    "    fig, axes = plt.subplots(len(emotions), samples_per_class, \n",
    "                            figsize=(12, 2*len(emotions)))\n",
    "    fig.suptitle(f'Sample Images from {data_dir}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "            \n",
    "        # Get list of images\n",
    "        images = [f for f in os.listdir(emotion_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        # Sample random images\n",
    "        sample_images = np.random.choice(images, min(samples_per_class, len(images)), replace=False)\n",
    "        \n",
    "        for j, img_name in enumerate(sample_images):\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "            \n",
    "            if j == 0:\n",
    "                axes[i, j].set_title(f'{emotion}\\n{img.shape}', fontweight='bold')\n",
    "            else:\n",
    "                axes[i, j].set_title(f'{img.shape}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run verification\n",
    "train_counts, test_counts = verify_dataset()\n",
    "\n",
    "# Display samples\n",
    "print(\"\\nDisplaying sample images from training set...\")\n",
    "display_samples(TRAIN_DIR, EMOTIONS, samples_per_class=3)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = pd.DataFrame({\n",
    "    'Emotion': EMOTIONS,\n",
    "    'Train Count': [train_counts.get(e, 0) for e in EMOTIONS],\n",
    "    'Test Count': [test_counts.get(e, 0) for e in EMOTIONS]\n",
    "})\n",
    "summary_df['Total'] = summary_df['Train Count'] + summary_df['Test Count']\n",
    "summary_df['Train %'] = (summary_df['Train Count'] / summary_df['Train Count'].sum() * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 1: Face Detection & Visual Check\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Option 1: Haar Cascade Face Detector\n",
    "class HaarFaceDetector:\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "    \n",
    "    def detect(self, image):\n",
    "        \"\"\"Detect faces in grayscale image\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "        \n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        return faces\n",
    "\n",
    "# Option 2: MediaPipe Face Detection\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    \n",
    "    class MediaPipeFaceDetector:\n",
    "        def __init__(self):\n",
    "            self.mp_face_detection = mp.solutions.face_detection\n",
    "            self.face_detection = self.mp_face_detection.FaceDetection(\n",
    "                model_selection=0,  # 0 for short-range detection\n",
    "                min_detection_confidence=0.5\n",
    "            )\n",
    "        \n",
    "        def detect(self, image):\n",
    "            \"\"\"Detect faces using MediaPipe\"\"\"\n",
    "            if len(image.shape) == 2:\n",
    "                # Convert grayscale to RGB\n",
    "                rgb_image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            else:\n",
    "                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            results = self.face_detection.process(rgb_image)\n",
    "            \n",
    "            if not results.detections:\n",
    "                return np.array([])\n",
    "            \n",
    "            # Convert MediaPipe detections to bounding boxes\n",
    "            h, w = image.shape[:2]\n",
    "            faces = []\n",
    "            \n",
    "            for detection in results.detections:\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x = int(bbox.xmin * w)\n",
    "                y = int(bbox.ymin * h)\n",
    "                width = int(bbox.width * w)\n",
    "                height = int(bbox.height * h)\n",
    "                faces.append([x, y, width, height])\n",
    "            \n",
    "            return np.array(faces)\n",
    "    \n",
    "    MEDIAPIPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MEDIAPIPE_AVAILABLE = False\n",
    "    print(\"MediaPipe not available, using Haar Cascade only\")\n",
    "\n",
    "# Initialize detector (prefer Haar for FER2013 as faces are already cropped)\n",
    "detector = HaarFaceDetector()\n",
    "print(\"‚úì Face detector initialized (Haar Cascade)\")\n",
    "\n",
    "def draw_faces(image, faces, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"Draw bounding boxes on detected faces\"\"\"\n",
    "    result = image.copy()\n",
    "    if len(result.shape) == 2:\n",
    "        result = cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(result, (x, y), (x+w, y+h), color, thickness)\n",
    "        cv2.putText(result, f'{w}x{h}', (x, y-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_face_detection(data_dir, emotions, samples_per_class=4):\n",
    "    \"\"\"Test face detection on samples from each emotion class\"\"\"\n",
    "    fig, axes = plt.subplots(len(emotions), samples_per_class, \n",
    "                            figsize=(15, 2.5*len(emotions)))\n",
    "    fig.suptitle('Face Detection Results (Green boxes = detected faces)', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    detection_stats = {}\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(emotion_path) \n",
    "                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        sample_images = np.random.choice(images, \n",
    "                                        min(samples_per_class, len(images)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        detected = 0\n",
    "        for j, img_name in enumerate(sample_images):\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = detector.detect(img)\n",
    "            \n",
    "            # Draw boxes\n",
    "            result = draw_faces(img, faces)\n",
    "            \n",
    "            # Display\n",
    "            axes[i, j].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "            axes[i, j].axis('off')\n",
    "            \n",
    "            # Title with detection info\n",
    "            if len(faces) > 0:\n",
    "                detected += 1\n",
    "                title = f'‚úì {len(faces)} face(s)'\n",
    "            else:\n",
    "                title = '‚úó No face'\n",
    "            \n",
    "            if j == 0:\n",
    "                axes[i, j].set_title(f'{emotion}\\n{title}', fontweight='bold')\n",
    "            else:\n",
    "                axes[i, j].set_title(title)\n",
    "        \n",
    "        detection_stats[emotion] = f\"{detected}/{samples_per_class}\"\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FACE DETECTION STATISTICS (Sample)\")\n",
    "    print(\"=\" * 60)\n",
    "    for emotion, stat in detection_stats.items():\n",
    "        print(f\"  {emotion:10s}: {stat} detected\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return detection_stats\n",
    "\n",
    "# Run face detection test\n",
    "print(\"\\nTesting face detection on training samples...\")\n",
    "detection_stats = test_face_detection(TRAIN_DIR, EMOTIONS, samples_per_class=4)\n",
    "\n",
    "# Analyze detection rate across entire dataset\n",
    "def analyze_detection_rate(data_dir, emotions, max_samples=200):\n",
    "    \"\"\"Analyze face detection rate across the dataset\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETECTION RATE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(emotion_path) \n",
    "                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        # Sample subset if too many images\n",
    "        if len(images) > max_samples:\n",
    "            images = np.random.choice(images, max_samples, replace=False)\n",
    "        \n",
    "        detected_count = 0\n",
    "        total_faces = 0\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            faces = detector.detect(img)\n",
    "            \n",
    "            if len(faces) > 0:\n",
    "                detected_count += 1\n",
    "                total_faces += len(faces)\n",
    "        \n",
    "        detection_rate = (detected_count / len(images)) * 100\n",
    "        avg_faces = total_faces / detected_count if detected_count > 0 else 0\n",
    "        \n",
    "        results[emotion] = {\n",
    "            'total': len(images),\n",
    "            'detected': detected_count,\n",
    "            'rate': detection_rate,\n",
    "            'avg_faces': avg_faces\n",
    "        }\n",
    "        \n",
    "        print(f\"  {emotion:10s}: {detected_count:3d}/{len(images):3d} \"\n",
    "              f\"({detection_rate:.1f}%) | Avg faces/img: {avg_faces:.2f}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze detection rate\n",
    "detection_analysis = analyze_detection_rate(TRAIN_DIR, EMOTIONS, max_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 2 - Track A: Landmark Feature Extraction\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class LandmarkFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize MediaPipe FaceMesh for landmark detection\"\"\"\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            static_image_mode=True,\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        \n",
    "    def crop_largest_face(self, image):\n",
    "        \"\"\"Crop the largest detected face from the image\"\"\"\n",
    "        faces = detector.detect(image)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # If no face detected, return the center crop\n",
    "            h, w = image.shape[:2]\n",
    "            size = min(h, w)\n",
    "            y = (h - size) // 2\n",
    "            x = (w - size) // 2\n",
    "            return image[y:y+size, x:x+size]\n",
    "        \n",
    "        # Get largest face\n",
    "        largest_face = faces[np.argmax([w*h for (x, y, w, h) in faces])]\n",
    "        x, y, w, h = largest_face\n",
    "        \n",
    "        # Add some padding\n",
    "        padding = int(0.1 * min(w, h))\n",
    "        x = max(0, x - padding)\n",
    "        y = max(0, y - padding)\n",
    "        w = min(image.shape[1] - x, w + 2*padding)\n",
    "        h = min(image.shape[0] - y, h + 2*padding)\n",
    "        \n",
    "        return image[y:y+h, x:x+w]\n",
    "    \n",
    "    def extract_landmarks(self, image):\n",
    "        \"\"\"Extract facial landmarks from image\"\"\"\n",
    "        # Crop face\n",
    "        face_crop = self.crop_largest_face(image)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        if len(face_crop.shape) == 2:\n",
    "            rgb_image = cv2.cvtColor(face_crop, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            rgb_image = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process with FaceMesh\n",
    "        results = self.face_mesh.process(rgb_image)\n",
    "        \n",
    "        if not results.multi_face_landmarks:\n",
    "            return None\n",
    "        \n",
    "        # Get landmarks\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        \n",
    "        # Extract coordinates\n",
    "        h, w = rgb_image.shape[:2]\n",
    "        coords = []\n",
    "        for landmark in landmarks.landmark:\n",
    "            coords.append([landmark.x * w, landmark.y * h])\n",
    "        \n",
    "        return np.array(coords)\n",
    "    \n",
    "    def normalize_landmarks(self, landmarks):\n",
    "        \"\"\"Normalize landmarks by centering and scaling\"\"\"\n",
    "        if landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        # Center landmarks\n",
    "        centroid = np.mean(landmarks, axis=0)\n",
    "        centered = landmarks - centroid\n",
    "        \n",
    "        # Calculate inter-pupillary distance for scaling\n",
    "        # Left eye: landmark 33, Right eye: landmark 263\n",
    "        left_eye = landmarks[33]\n",
    "        right_eye = landmarks[263]\n",
    "        ipd = np.linalg.norm(left_eye - right_eye)\n",
    "        \n",
    "        if ipd < 1e-6:  # Avoid division by zero\n",
    "            ipd = 1.0\n",
    "        \n",
    "        # Scale by IPD\n",
    "        normalized = centered / ipd\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def compute_geometric_features(self, landmarks):\n",
    "        \"\"\"Compute geometric features from landmarks\"\"\"\n",
    "        if landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        # Eye Aspect Ratio (EAR) - average for both eyes\n",
    "        def eye_aspect_ratio(eye_points):\n",
    "            # Vertical distances\n",
    "            v1 = np.linalg.norm(eye_points[1] - eye_points[5])\n",
    "            v2 = np.linalg.norm(eye_points[2] - eye_points[4])\n",
    "            # Horizontal distance\n",
    "            h = np.linalg.norm(eye_points[0] - eye_points[3])\n",
    "            ear = (v1 + v2) / (2.0 * h)\n",
    "            return ear\n",
    "        \n",
    "        # Left eye landmarks (simplified)\n",
    "        left_eye = landmarks[[33, 160, 158, 133, 153, 144]]\n",
    "        # Right eye landmarks\n",
    "        right_eye = landmarks[[362, 385, 387, 263, 373, 380]]\n",
    "        \n",
    "        left_ear = eye_aspect_ratio(left_eye)\n",
    "        right_ear = eye_aspect_ratio(right_eye)\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        \n",
    "        # Mouth Aspect Ratio (MAR)\n",
    "        # Upper lip: 13, Lower lip: 14, Left corner: 78, Right corner: 308\n",
    "        mouth_top = landmarks[13]\n",
    "        mouth_bottom = landmarks[14]\n",
    "        mouth_left = landmarks[78]\n",
    "        mouth_right = landmarks[308]\n",
    "        \n",
    "        mouth_height = np.linalg.norm(mouth_top - mouth_bottom)\n",
    "        mouth_width = np.linalg.norm(mouth_left - mouth_right)\n",
    "        mar = mouth_height / (mouth_width + 1e-6)\n",
    "        \n",
    "        return np.array([avg_ear, left_ear, right_ear, mar])\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"Extract complete feature vector from image\"\"\"\n",
    "        # Get landmarks\n",
    "        landmarks = self.extract_landmarks(image)\n",
    "        \n",
    "        if landmarks is None:\n",
    "            return None\n",
    "        \n",
    "        # Normalize landmarks\n",
    "        normalized = self.normalize_landmarks(landmarks)\n",
    "        \n",
    "        # Flatten normalized landmarks\n",
    "        flattened_landmarks = normalized.flatten()\n",
    "        \n",
    "        # Compute geometric features\n",
    "        geometric_features = self.compute_geometric_features(landmarks)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.concatenate([flattened_landmarks, geometric_features])\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Initialize feature extractor\n",
    "print(\"Initializing Landmark Feature Extractor...\")\n",
    "landmark_extractor = LandmarkFeatureExtractor()\n",
    "print(\"‚úì Landmark Feature Extractor initialized\")\n",
    "\n",
    "# Test on a few samples\n",
    "def test_landmark_extraction(data_dir, emotions, samples=2):\n",
    "    \"\"\"Test landmark extraction on sample images\"\"\"\n",
    "    fig, axes = plt.subplots(len(emotions), samples*2, \n",
    "                            figsize=(12, 2*len(emotions)))\n",
    "    fig.suptitle('Landmark Extraction Test', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(emotion_path) \n",
    "                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        sample_images = np.random.choice(images, min(samples, len(images)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        for j, img_name in enumerate(sample_images):\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            landmarks = landmark_extractor.extract_landmarks(img)\n",
    "            \n",
    "            # Original image\n",
    "            axes[i, j*2].imshow(img, cmap='gray')\n",
    "            axes[i, j*2].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j*2].set_title(f'{emotion}\\nOriginal', fontweight='bold')\n",
    "            else:\n",
    "                axes[i, j*2].set_title('Original')\n",
    "            \n",
    "            # Image with landmarks\n",
    "            face_crop = landmark_extractor.crop_largest_face(img)\n",
    "            if len(face_crop.shape) == 2:\n",
    "                vis_img = cv2.cvtColor(face_crop, cv2.COLOR_GRAY2BGR)\n",
    "            else:\n",
    "                vis_img = face_crop.copy()\n",
    "            \n",
    "            if landmarks is not None:\n",
    "                for (x, y) in landmarks:\n",
    "                    cv2.circle(vis_img, (int(x), int(y)), 1, (0, 255, 0), -1)\n",
    "                title = f'‚úì {len(landmarks)} landmarks'\n",
    "            else:\n",
    "                title = '‚úó No landmarks'\n",
    "            \n",
    "            axes[i, j*2+1].imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))\n",
    "            axes[i, j*2+1].axis('off')\n",
    "            axes[i, j*2+1].set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nTesting landmark extraction...\")\n",
    "test_landmark_extraction(TRAIN_DIR, EMOTIONS, samples=2)\n",
    "\n",
    "# Extract features for entire dataset\n",
    "def extract_dataset_features(data_dir, emotions, extractor, save_path=None):\n",
    "    \"\"\"Extract features from entire dataset\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"\\nExtracting features from {data_dir}...\")\n",
    "    \n",
    "    for emotion_idx, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(emotion_path) \n",
    "                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        print(f\"  Processing {emotion}... ({len(images)} images)\")\n",
    "        \n",
    "        for img_name in tqdm(images, desc=f\"  {emotion}\", leave=False):\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            features = extractor.extract_features(img)\n",
    "            \n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(emotion_idx)\n",
    "            else:\n",
    "                failed_count += 1\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"\\n‚úì Extracted {len(X)} feature vectors\")\n",
    "    print(f\"  Feature dimension: {X.shape[1]}\")\n",
    "    print(f\"  Failed extractions: {failed_count}\")\n",
    "    \n",
    "    if save_path:\n",
    "        np.savez(save_path, X=X, y=y, emotions=emotions)\n",
    "        print(f\"  Saved to: {save_path}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Extract features from train and test sets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTING LANDMARK FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train_landmarks, y_train = extract_dataset_features(\n",
    "    TRAIN_DIR, EMOTIONS, landmark_extractor, \n",
    "    save_path='features_train_landmarks.npz'\n",
    ")\n",
    "\n",
    "X_test_landmarks, y_test = extract_dataset_features(\n",
    "    TEST_DIR, EMOTIONS, landmark_extractor,\n",
    "    save_path='features_test_landmarks.npz'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LANDMARK FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train set: {X_train_landmarks.shape}\")\n",
    "print(f\"Test set:  {X_test_landmarks.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 2 - Track B: CNN Deep Feature Extraction\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CNNFeatureExtractor:\n",
    "    def __init__(self, target_size=(224, 224)):\n",
    "        \"\"\"Initialize pretrained CNN for feature extraction\"\"\"\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Load pretrained MobileNetV2 without top layer\n",
    "        print(\"Loading pretrained MobileNetV2...\")\n",
    "        base_model = MobileNetV2(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(*target_size, 3),\n",
    "            pooling='avg'  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Use the model as feature extractor\n",
    "        self.model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "        \n",
    "        # Freeze the model (no training)\n",
    "        self.model.trainable = False\n",
    "        \n",
    "        print(f\"‚úì CNN Feature Extractor initialized\")\n",
    "        print(f\"  Output feature dimension: {self.model.output_shape[1]}\")\n",
    "    \n",
    "    def crop_largest_face(self, image):\n",
    "        \"\"\"Crop the largest detected face from the image\"\"\"\n",
    "        faces = detector.detect(image)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # If no face detected, return the center crop\n",
    "            h, w = image.shape[:2]\n",
    "            size = min(h, w)\n",
    "            y = (h - size) // 2\n",
    "            x = (w - size) // 2\n",
    "            return image[y:y+size, x:x+size]\n",
    "        \n",
    "        # Get largest face\n",
    "        largest_face = faces[np.argmax([w*h for (x, y, w, h) in faces])]\n",
    "        x, y, w, h = largest_face\n",
    "        \n",
    "        # Add some padding\n",
    "        padding = int(0.1 * min(w, h))\n",
    "        x = max(0, x - padding)\n",
    "        y = max(0, y - padding)\n",
    "        w = min(image.shape[1] - x, w + 2*padding)\n",
    "        h = min(image.shape[0] - y, h + 2*padding)\n",
    "        \n",
    "        return image[y:y+h, x:x+w]\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for CNN input\"\"\"\n",
    "        # Crop face\n",
    "        face_crop = self.crop_largest_face(image)\n",
    "        \n",
    "        # Resize to target size\n",
    "        resized = cv2.resize(face_crop, self.target_size)\n",
    "        \n",
    "        # Convert grayscale to 3 channels\n",
    "        if len(resized.shape) == 2:\n",
    "            rgb_image = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        return rgb_image\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"Extract deep features from a single image\"\"\"\n",
    "        # Preprocess\n",
    "        processed = self.preprocess_image(image)\n",
    "        \n",
    "        # Add batch dimension and preprocess for MobileNetV2\n",
    "        batch = np.expand_dims(processed, axis=0)\n",
    "        batch = preprocess_input(batch)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.model.predict(batch, verbose=0)\n",
    "        \n",
    "        return features[0]  # Remove batch dimension\n",
    "    \n",
    "    def extract_features_batch(self, images, batch_size=32):\n",
    "        \"\"\"Extract features from multiple images efficiently\"\"\"\n",
    "        # Preprocess all images\n",
    "        processed_images = []\n",
    "        for img in images:\n",
    "            processed = self.preprocess_image(img)\n",
    "            processed_images.append(processed)\n",
    "        \n",
    "        # Convert to array and preprocess\n",
    "        batch = np.array(processed_images)\n",
    "        batch = preprocess_input(batch)\n",
    "        \n",
    "        # Extract features in batches\n",
    "        features = self.model.predict(batch, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Initialize CNN feature extractor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING CNN FEATURE EXTRACTOR\")\n",
    "print(\"=\"*60)\n",
    "cnn_extractor = CNNFeatureExtractor(target_size=(224, 224))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test on a few samples\n",
    "def test_cnn_extraction(data_dir, emotions, samples=2):\n",
    "    \"\"\"Test CNN feature extraction on sample images\"\"\"\n",
    "    fig, axes = plt.subplots(len(emotions), samples*2, \n",
    "                            figsize=(12, 2*len(emotions)))\n",
    "    fig.suptitle('CNN Feature Extraction Test', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(emotion_path) \n",
    "                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        sample_images = np.random.choice(images, min(samples, len(images)), \n",
    "                                        replace=False)\n",
    "        \n",
    "        for j, img_name in enumerate(sample_images):\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Extract features\n",
    "            features = cnn_extractor.extract_features(img)\n",
    "            \n",
    "            # Original image\n",
    "            axes[i, j*2].imshow(img, cmap='gray')\n",
    "            axes[i, j*2].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j*2].set_title(f'{emotion}\\nOriginal (48x48)', \n",
    "                                      fontweight='bold')\n",
    "            else:\n",
    "                axes[i, j*2].set_title('Original (48x48)')\n",
    "            \n",
    "            # Preprocessed image\n",
    "            preprocessed = cnn_extractor.preprocess_image(img)\n",
    "            axes[i, j*2+1].imshow(preprocessed)\n",
    "            axes[i, j*2+1].axis('off')\n",
    "            axes[i, j*2+1].set_title(f'CNN Input (224x224)\\n‚úì {len(features)} features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nTesting CNN feature extraction...\")\n",
    "test_cnn_extraction(TRAIN_DIR, EMOTIONS, samples=2)\n",
    "\n",
    "# Extract features for entire dataset\n",
    "def extract_dataset_cnn_features(data_dir, emotions, extractor, \n",
    "                                 batch_size=32, save_path=None):\n",
    "    \"\"\"Extract CNN features from entire dataset\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"\\nExtracting CNN features from {data_dir}...\")\n",
    "    \n",
    "    for emotion_idx, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        image_files = [f for f in os.listdir(emotion_path) \n",
    "                      if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        print(f\"  Processing {emotion}... ({len(image_files)} images)\")\n",
    "        \n",
    "        # Process in batches for efficiency\n",
    "        for i in tqdm(range(0, len(image_files), batch_size), \n",
    "                     desc=f\"  {emotion}\", leave=False):\n",
    "            batch_files = image_files[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            for img_name in batch_files:\n",
    "                img_path = os.path.join(emotion_path, img_name)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                batch_images.append(img)\n",
    "            \n",
    "            # Extract features for batch\n",
    "            batch_features = extractor.extract_features_batch(batch_images, \n",
    "                                                             batch_size=len(batch_images))\n",
    "            \n",
    "            X.extend(batch_features)\n",
    "            y.extend([emotion_idx] * len(batch_features))\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"\\n‚úì Extracted {len(X)} feature vectors\")\n",
    "    print(f\"  Feature dimension: {X.shape[1]}\")\n",
    "    \n",
    "    if save_path:\n",
    "        np.savez(save_path, X=X, y=y, emotions=emotions)\n",
    "        print(f\"  Saved to: {save_path}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Extract features from train and test sets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTING CNN FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train_cnn, y_train_cnn = extract_dataset_cnn_features(\n",
    "    TRAIN_DIR, EMOTIONS, cnn_extractor, \n",
    "    batch_size=32,\n",
    "    save_path='features_train_cnn.npz'\n",
    ")\n",
    "\n",
    "X_test_cnn, y_test_cnn = extract_dataset_cnn_features(\n",
    "    TEST_DIR, EMOTIONS, cnn_extractor,\n",
    "    batch_size=32,\n",
    "    save_path='features_test_cnn.npz'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CNN FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train set: {X_train_cnn.shape}\")\n",
    "print(f\"Test set:  {X_test_cnn.shape}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify labels match\n",
    "assert np.array_equal(y_train, y_train_cnn), \"Train labels mismatch!\"\n",
    "assert np.array_equal(y_test, y_test_cnn), \"Test labels mismatch!\"\n",
    "print(\"‚úì Labels verified - all sets aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training - Both Tracks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dictionary to store all results\n",
    "results = {}\n",
    "\n",
    "# ============================================================================\n",
    "# TRACK A: LANDMARK FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"TRACK A: LANDMARK FEATURES\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Standardize features for landmark-based models\n",
    "print(\"\\nStandardizing landmark features...\")\n",
    "scaler_landmarks = StandardScaler()\n",
    "X_train_landmarks_scaled = scaler_landmarks.fit_transform(X_train_landmarks)\n",
    "X_test_landmarks_scaled = scaler_landmarks.transform(X_test_landmarks)\n",
    "print(\"‚úì Features standardized\")\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler_landmarks.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_landmarks, f)\n",
    "\n",
    "# Model 1: SVM with RBF kernel (Landmark features)\n",
    "print(\"\\n[1/4] Training SVM on Landmark features...\")\n",
    "print(\"  Hyperparameters: C=1.0, kernel='rbf', gamma='scale'\")\n",
    "start_time = time.time()\n",
    "\n",
    "svm_landmarks = SVC(\n",
    "    C=1.0,\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=False\n",
    ")\n",
    "svm_landmarks.fit(X_train_landmarks_scaled, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "y_pred_train = svm_landmarks.predict(X_train_landmarks_scaled)\n",
    "y_pred_test = svm_landmarks.predict(X_test_landmarks_scaled)\n",
    "\n",
    "results['SVM_Landmarks'] = {\n",
    "    'model': svm_landmarks,\n",
    "    'train_acc': accuracy_score(y_train, y_pred_train),\n",
    "    'test_acc': accuracy_score(y_test, y_pred_test),\n",
    "    'train_f1': f1_score(y_train, y_pred_train, average='macro'),\n",
    "    'test_f1': f1_score(y_test, y_pred_test, average='macro'),\n",
    "    'train_time': train_time,\n",
    "    'y_pred': y_pred_test,\n",
    "    'feature_type': 'Landmarks'\n",
    "}\n",
    "\n",
    "print(f\"  ‚úì Training completed in {train_time:.2f}s\")\n",
    "print(f\"  Train Accuracy: {results['SVM_Landmarks']['train_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy:  {results['SVM_Landmarks']['test_acc']:.4f}\")\n",
    "print(f\"  Test Macro-F1:  {results['SVM_Landmarks']['test_f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_svm_landmarks.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_landmarks, f)\n",
    "\n",
    "# Model 2: Random Forest (Landmark features)\n",
    "print(\"\\n[2/4] Training Random Forest on Landmark features...\")\n",
    "print(\"  Hyperparameters: n_estimators=100, max_depth=20\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_landmarks = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_landmarks.fit(X_train_landmarks, y_train)  # RF doesn't need standardization\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "y_pred_train = rf_landmarks.predict(X_train_landmarks)\n",
    "y_pred_test = rf_landmarks.predict(X_test_landmarks)\n",
    "\n",
    "results['RF_Landmarks'] = {\n",
    "    'model': rf_landmarks,\n",
    "    'train_acc': accuracy_score(y_train, y_pred_train),\n",
    "    'test_acc': accuracy_score(y_test, y_pred_test),\n",
    "    'train_f1': f1_score(y_train, y_pred_train, average='macro'),\n",
    "    'test_f1': f1_score(y_test, y_pred_test, average='macro'),\n",
    "    'train_time': train_time,\n",
    "    'y_pred': y_pred_test,\n",
    "    'feature_type': 'Landmarks'\n",
    "}\n",
    "\n",
    "print(f\"  ‚úì Training completed in {train_time:.2f}s\")\n",
    "print(f\"  Train Accuracy: {results['RF_Landmarks']['train_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy:  {results['RF_Landmarks']['test_acc']:.4f}\")\n",
    "print(f\"  Test Macro-F1:  {results['RF_Landmarks']['test_f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_rf_landmarks.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_landmarks, f)\n",
    "\n",
    "# ============================================================================\n",
    "# TRACK B: CNN FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"TRACK B: CNN DEEP FEATURES\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Standardize features for CNN-based models\n",
    "print(\"\\nStandardizing CNN features...\")\n",
    "scaler_cnn = StandardScaler()\n",
    "X_train_cnn_scaled = scaler_cnn.fit_transform(X_train_cnn)\n",
    "X_test_cnn_scaled = scaler_cnn.transform(X_test_cnn)\n",
    "print(\"‚úì Features standardized\")\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_cnn, f)\n",
    "\n",
    "# Model 3: SVM with RBF kernel (CNN features)\n",
    "print(\"\\n[3/4] Training SVM on CNN features...\")\n",
    "print(\"  Hyperparameters: C=10.0, kernel='rbf', gamma='scale'\")\n",
    "start_time = time.time()\n",
    "\n",
    "svm_cnn = SVC(\n",
    "    C=10.0,\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=False\n",
    ")\n",
    "svm_cnn.fit(X_train_cnn_scaled, y_train_cnn)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "y_pred_train = svm_cnn.predict(X_train_cnn_scaled)\n",
    "y_pred_test = svm_cnn.predict(X_test_cnn_scaled)\n",
    "\n",
    "results['SVM_CNN'] = {\n",
    "    'model': svm_cnn,\n",
    "    'train_acc': accuracy_score(y_train_cnn, y_pred_train),\n",
    "    'test_acc': accuracy_score(y_test_cnn, y_pred_test),\n",
    "    'train_f1': f1_score(y_train_cnn, y_pred_train, average='macro'),\n",
    "    'test_f1': f1_score(y_test_cnn, y_pred_test, average='macro'),\n",
    "    'train_time': train_time,\n",
    "    'y_pred': y_pred_test,\n",
    "    'feature_type': 'CNN'\n",
    "}\n",
    "\n",
    "print(f\"  ‚úì Training completed in {train_time:.2f}s\")\n",
    "print(f\"  Train Accuracy: {results['SVM_CNN']['train_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy:  {results['SVM_CNN']['test_acc']:.4f}\")\n",
    "print(f\"  Test Macro-F1:  {results['SVM_CNN']['test_f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_svm_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_cnn, f)\n",
    "\n",
    "# Model 4: Logistic Regression (CNN features)\n",
    "print(\"\\n[4/4] Training Logistic Regression on CNN features...\")\n",
    "print(\"  Hyperparameters: C=1.0, max_iter=1000, multi_class='multinomial'\")\n",
    "start_time = time.time()\n",
    "\n",
    "logreg_cnn = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg_cnn.fit(X_train_cnn_scaled, y_train_cnn)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "y_pred_train = logreg_cnn.predict(X_train_cnn_scaled)\n",
    "y_pred_test = logreg_cnn.predict(X_test_cnn_scaled)\n",
    "\n",
    "results['LogReg_CNN'] = {\n",
    "    'model': logreg_cnn,\n",
    "    'train_acc': accuracy_score(y_train_cnn, y_pred_train),\n",
    "    'test_acc': accuracy_score(y_test_cnn, y_pred_test),\n",
    "    'train_f1': f1_score(y_train_cnn, y_pred_train, average='macro'),\n",
    "    'test_f1': f1_score(y_test_cnn, y_pred_test, average='macro'),\n",
    "    'train_time': train_time,\n",
    "    'y_pred': y_pred_test,\n",
    "    'feature_type': 'CNN'\n",
    "}\n",
    "\n",
    "print(f\"  ‚úì Training completed in {train_time:.2f}s\")\n",
    "print(f\"  Train Accuracy: {results['LogReg_CNN']['train_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy:  {results['LogReg_CNN']['test_acc']:.4f}\")\n",
    "print(f\"  Test Macro-F1:  {results['LogReg_CNN']['test_f1']:.4f}\")\n",
    "\n",
    "# Save model\n",
    "with open('model_logreg_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_cnn, f)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "for model_name, result in results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Features': result['feature_type'],\n",
    "        'Train Acc': f\"{result['train_acc']:.4f}\",\n",
    "        'Test Acc': f\"{result['test_acc']:.4f}\",\n",
    "        'Test F1': f\"{result['test_f1']:.4f}\",\n",
    "        'Time (s)': f\"{result['train_time']:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['test_acc'])\n",
    "best_model_info = results[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_model_info['test_acc']:.4f}\")\n",
    "print(f\"   Test Macro-F1: {best_model_info['test_f1']:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save all results\n",
    "with open('training_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(\"\\n‚úì All models and results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 3: Evaluation & Reflection\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             precision_score, recall_score, f1_score)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to evaluate a single model\n",
    "def evaluate_model(model_name, y_true, y_pred, emotions):\n",
    "    \"\"\"Comprehensive evaluation of a single model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL METRICS:\")\n",
    "    print(f\"  Accuracy:        {accuracy:.4f}\")\n",
    "    print(f\"  Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"  Macro Recall:    {macro_recall:.4f}\")\n",
    "    print(f\"  Macro F1-Score:  {macro_f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìã CLASSIFICATION REPORT:\")\n",
    "    print(\"-\"*60)\n",
    "    report = classification_report(y_true, y_pred, \n",
    "                                   target_names=emotions,\n",
    "                                   digits=4)\n",
    "    print(report)\n",
    "    \n",
    "    # Get per-class metrics for analysis\n",
    "    report_dict = classification_report(y_true, y_pred, \n",
    "                                       target_names=emotions,\n",
    "                                       output_dict=True)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=emotions, yticklabels=emotions,\n",
    "               cbar_kws={'label': 'Count'})\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze confusion patterns\n",
    "    print(f\"\\nüîç CONFUSION ANALYSIS:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Normalize confusion matrix by row (true labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Find top confusions (excluding diagonal)\n",
    "    confusions = []\n",
    "    for i in range(len(emotions)):\n",
    "        for j in range(len(emotions)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusions.append({\n",
    "                    'true': emotions[i],\n",
    "                    'pred': emotions[j],\n",
    "                    'count': cm[i, j],\n",
    "                    'rate': cm_normalized[i, j]\n",
    "                })\n",
    "    \n",
    "    # Sort by count\n",
    "    confusions = sorted(confusions, key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    print(\"Top confusion patterns:\")\n",
    "    for conf in confusions[:5]:\n",
    "        print(f\"  ‚Ä¢ {conf['true']:8s} ‚Üí {conf['pred']:8s}: \"\n",
    "              f\"{conf['count']:3d} errors ({conf['rate']*100:.1f}%)\")\n",
    "    \n",
    "    # Identify hardest classes (lowest F1)\n",
    "    class_f1 = [(emotions[i], report_dict[emotions[i]]['f1-score']) \n",
    "                for i in range(len(emotions))]\n",
    "    class_f1 = sorted(class_f1, key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  HARDEST CLASSES (by F1-score):\")\n",
    "    for emotion, f1 in class_f1[:3]:\n",
    "        precision = report_dict[emotion]['precision']\n",
    "        recall = report_dict[emotion]['recall']\n",
    "        support = report_dict[emotion]['support']\n",
    "        print(f\"  ‚Ä¢ {emotion:8s}: F1={f1:.4f} (P={precision:.4f}, \"\n",
    "              f\"R={recall:.4f}, N={support:.0f})\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'report_dict': report_dict,\n",
    "        'top_confusions': confusions[:5],\n",
    "        'hardest_classes': class_f1[:3]\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model_info in results.items():\n",
    "    if model_info['feature_type'] == 'Landmarks':\n",
    "        y_true = y_test\n",
    "    else:\n",
    "        y_true = y_test_cnn\n",
    "    \n",
    "    eval_result = evaluate_model(model_name, y_true, \n",
    "                                 model_info['y_pred'], EMOTIONS)\n",
    "    evaluation_results[model_name] = eval_result\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARATIVE SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARATIVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for model_name in results.keys():\n",
    "    eval_res = evaluation_results[model_name]\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': f\"{eval_res['accuracy']:.4f}\",\n",
    "        'Macro-Precision': f\"{eval_res['macro_precision']:.4f}\",\n",
    "        'Macro-Recall': f\"{eval_res['macro_recall']:.4f}\",\n",
    "        'Macro-F1': f\"{eval_res['macro_f1']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# KEY INSIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Best performing model\n",
    "best_model = max(evaluation_results.keys(), \n",
    "                key=lambda k: evaluation_results[k]['accuracy'])\n",
    "best_acc = evaluation_results[best_model]['accuracy']\n",
    "\n",
    "print(f\"\\n1. üèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {best_acc:.4f}\")\n",
    "print(f\"   ‚Ä¢ Feature Type: {results[best_model]['feature_type']}\")\n",
    "if results[best_model]['feature_type'] == 'CNN':\n",
    "    print(f\"   ‚Ä¢ Reason: Deep features from pretrained CNN capture\")\n",
    "    print(f\"     discriminative patterns better than hand-crafted landmarks\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Reason: Facial landmark geometry effectively captures\")\n",
    "    print(f\"     emotion-specific facial configurations\")\n",
    "\n",
    "# 2. Hardest classes across all models\n",
    "print(f\"\\n2. ‚ö†Ô∏è  HARDEST CLASSES (most commonly confused):\")\n",
    "all_hardest = {}\n",
    "for model_name, eval_res in evaluation_results.items():\n",
    "    for emotion, f1 in eval_res['hardest_classes']:\n",
    "        if emotion not in all_hardest:\n",
    "            all_hardest[emotion] = []\n",
    "        all_hardest[emotion].append(f1)\n",
    "\n",
    "avg_f1_by_class = {k: np.mean(v) for k, v in all_hardest.items()}\n",
    "hardest_overall = sorted(avg_f1_by_class.items(), key=lambda x: x[1])[:3]\n",
    "\n",
    "for emotion, avg_f1 in hardest_overall:\n",
    "    print(f\"   ‚Ä¢ {emotion:8s}: Average F1 = {avg_f1:.4f}\")\n",
    "    # Get common confusions for this class\n",
    "    for model_name, eval_res in evaluation_results.items():\n",
    "        for conf in eval_res['top_confusions']:\n",
    "            if conf['true'] == emotion:\n",
    "                print(f\"     ‚îî‚îÄ Often confused with '{conf['pred']}' \"\n",
    "                      f\"({conf['count']} times in {model_name})\")\n",
    "                break\n",
    "\n",
    "# 3. Feature comparison\n",
    "print(f\"\\n3. üìä FEATURE TYPE COMPARISON:\")\n",
    "landmark_models = [k for k, v in results.items() if v['feature_type'] == 'Landmarks']\n",
    "cnn_models = [k for k, v in results.items() if v['feature_type'] == 'CNN']\n",
    "\n",
    "landmark_avg = np.mean([evaluation_results[k]['accuracy'] for k in landmark_models])\n",
    "cnn_avg = np.mean([evaluation_results[k]['accuracy'] for k in cnn_models])\n",
    "\n",
    "print(f\"   ‚Ä¢ Landmark features: Avg accuracy = {landmark_avg:.4f}\")\n",
    "print(f\"   ‚Ä¢ CNN features:      Avg accuracy = {cnn_avg:.4f}\")\n",
    "print(f\"   ‚Ä¢ Difference:        {abs(cnn_avg - landmark_avg):.4f}\")\n",
    "\n",
    "# 4. Common confusion patterns\n",
    "print(f\"\\n4. üîÑ MOST COMMON CONFUSION PATTERNS:\")\n",
    "all_confusions = {}\n",
    "for model_name, eval_res in evaluation_results.items():\n",
    "    for conf in eval_res['top_confusions']:\n",
    "        key = f\"{conf['true']} ‚Üí {conf['pred']}\"\n",
    "        if key not in all_confusions:\n",
    "            all_confusions[key] = 0\n",
    "        all_confusions[key] += conf['count']\n",
    "\n",
    "top_confusions = sorted(all_confusions.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "for pattern, total_count in top_confusions:\n",
    "    print(f\"   ‚Ä¢ {pattern}: {total_count} total errors across all models\")\n",
    "\n",
    "# 5. Potential improvements\n",
    "print(f\"\\n5. üí° POTENTIAL IMPROVEMENTS:\")\n",
    "print(f\"   ‚Ä¢ Data Augmentation: Apply rotation, flipping, brightness\")\n",
    "print(f\"     adjustments to increase training diversity\")\n",
    "print(f\"   ‚Ä¢ Ensemble Methods: Combine predictions from multiple models\")\n",
    "print(f\"   ‚Ä¢ Fine-tuning CNN: Train the CNN backbone on FER2013 instead\")\n",
    "print(f\"     of just using as feature extractor\")\n",
    "print(f\"   ‚Ä¢ Address class imbalance: Use weighted loss or oversampling\")\n",
    "print(f\"   ‚Ä¢ Deeper architecture: Try ResNet or EfficientNet for features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save evaluation results\n",
    "with open('evaluation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)\n",
    "print(\"\\n‚úì Evaluation results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassification Analysis\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def analyze_misclassifications(model_name, y_true, y_pred, \n",
    "                               data_dir, emotions, num_examples=6):\n",
    "    \"\"\"Analyze and visualize misclassified examples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"MISCLASSIFICATION ANALYSIS - {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find misclassified indices\n",
    "    misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "    print(f\"\\nTotal misclassifications: {len(misclassified_indices)}\")\n",
    "    print(f\"Error rate: {len(misclassified_indices)/len(y_true)*100:.2f}%\")\n",
    "    \n",
    "    # Get all image paths\n",
    "    all_image_paths = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for emotion_idx, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = sorted([f for f in os.listdir(emotion_path) \n",
    "                        if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        \n",
    "        for img_name in images:\n",
    "            all_image_paths.append(os.path.join(emotion_path, img_name))\n",
    "            all_labels.append(emotion_idx)\n",
    "    \n",
    "    # Sample interesting misclassifications\n",
    "    # Prioritize: high confidence errors, diverse error types\n",
    "    misclass_info = []\n",
    "    for idx in misclassified_indices:\n",
    "        true_label = y_true[idx]\n",
    "        pred_label = y_pred[idx]\n",
    "        misclass_info.append({\n",
    "            'idx': idx,\n",
    "            'true': true_label,\n",
    "            'pred': pred_label,\n",
    "            'true_name': emotions[true_label],\n",
    "            'pred_name': emotions[pred_label]\n",
    "        })\n",
    "    \n",
    "    # Sample diverse examples (different error types)\n",
    "    unique_errors = {}\n",
    "    for info in misclass_info:\n",
    "        key = (info['true'], info['pred'])\n",
    "        if key not in unique_errors:\n",
    "            unique_errors[key] = []\n",
    "        unique_errors[key].append(info)\n",
    "    \n",
    "    # Select examples\n",
    "    selected_examples = []\n",
    "    for key, examples in unique_errors.items():\n",
    "        if len(selected_examples) < num_examples:\n",
    "            selected_examples.extend(examples[:1])\n",
    "    \n",
    "    # Fill remaining slots with random samples\n",
    "    if len(selected_examples) < num_examples:\n",
    "        remaining = [e for e in misclass_info if e not in selected_examples]\n",
    "        np.random.shuffle(remaining)\n",
    "        selected_examples.extend(remaining[:num_examples - len(selected_examples)])\n",
    "    \n",
    "    selected_examples = selected_examples[:num_examples]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    fig.suptitle(f'Misclassified Examples - {model_name}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, example in enumerate(selected_examples):\n",
    "        img_path = all_image_paths[example['idx']]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        title = (f\"True: {example['true_name']}\\n\"\n",
    "                f\"Predicted: {example['pred_name']}\")\n",
    "        axes[i].set_title(title, fontsize=10, color='red', fontweight='bold')\n",
    "        \n",
    "        # Analysis comment\n",
    "        comment = analyze_error_cause(example['true_name'], \n",
    "                                      example['pred_name'])\n",
    "        axes[i].text(0.5, -0.15, comment, \n",
    "                    transform=axes[i].transAxes,\n",
    "                    ha='center', fontsize=8, \n",
    "                    style='italic', wrap=True)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(selected_examples), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\nüîç EXAMPLE ANALYSIS:\")\n",
    "    print(\"-\"*60)\n",
    "    for i, example in enumerate(selected_examples, 1):\n",
    "        print(f\"\\n{i}. {example['true_name']} ‚Üí {example['pred_name']}\")\n",
    "        print(f\"   Likely cause: {analyze_error_cause(example['true_name'], example['pred_name'])}\")\n",
    "\n",
    "def analyze_error_cause(true_emotion, pred_emotion):\n",
    "    \"\"\"Provide likely cause for misclassification\"\"\"\n",
    "    # Common confusion patterns and their causes\n",
    "    confusions = {\n",
    "        ('happy', 'neutral'): \"Subtle smile, weak expression intensity\",\n",
    "        ('neutral', 'sad'): \"Relaxed face resembles slight sadness\",\n",
    "        ('fear', 'surprise'): \"Similar eye widening, mouth opening\",\n",
    "        ('angry', 'disgust'): \"Shared eyebrow lowering, nose wrinkling\",\n",
    "        ('sad', 'neutral'): \"Mild sadness hard to distinguish from neutral\",\n",
    "        ('surprise', 'fear'): \"Both show wide eyes, raised eyebrows\",\n",
    "        ('disgust', 'angry'): \"Similar facial muscle tension patterns\",\n",
    "        ('happy', 'surprise'): \"Mouth opening in both expressions\",\n",
    "        ('neutral', 'happy'): \"Very subtle positive expression\",\n",
    "        ('fear', 'sad'): \"Both show eye tension and downturned features\"\n",
    "    }\n",
    "    \n",
    "    key = (true_emotion, pred_emotion)\n",
    "    if key in confusions:\n",
    "        return confusions[key]\n",
    "    else:\n",
    "        return \"Ambiguous expression or poor image quality\"\n",
    "\n",
    "# Analyze misclassifications for best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['test_acc'])\n",
    "\n",
    "if results[best_model_name]['feature_type'] == 'Landmarks':\n",
    "    y_true_best = y_test\n",
    "    data_dir_best = TEST_DIR\n",
    "else:\n",
    "    y_true_best = y_test_cnn\n",
    "    data_dir_best = TEST_DIR\n",
    "\n",
    "analyze_misclassifications(\n",
    "    best_model_name,\n",
    "    y_true_best,\n",
    "    results[best_model_name]['y_pred'],\n",
    "    data_dir_best,\n",
    "    EMOTIONS,\n",
    "    num_examples=6\n",
    ")\n",
    "\n",
    "# Compare misclassifications across models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISCLASSIFICATION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, model_info in results.items():\n",
    "    if model_info['feature_type'] == 'Landmarks':\n",
    "        y_true_comp = y_test\n",
    "    else:\n",
    "        y_true_comp = y_test_cnn\n",
    "    \n",
    "    misclass_count = np.sum(y_true_comp != model_info['y_pred'])\n",
    "    error_rate = misclass_count / len(y_true_comp) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Misclassifications: {misclass_count}/{len(y_true_comp)}\")\n",
    "    print(f\"  Error rate: {error_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c454145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Deployment - Predict Function\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EmotionPredictor:\n",
    "    \"\"\"Complete emotion prediction pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_path, feature_extractor, \n",
    "                 emotions, feature_type='CNN'):\n",
    "        \"\"\"\n",
    "        Initialize emotion predictor\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to saved model (.pkl)\n",
    "            scaler_path: Path to saved scaler (.pkl)\n",
    "            feature_extractor: Feature extraction object\n",
    "            emotions: List of emotion labels\n",
    "            feature_type: 'CNN' or 'Landmarks'\n",
    "        \"\"\"\n",
    "        self.emotions = emotions\n",
    "        self.feature_type = feature_type\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        # Load model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(f\"‚úì Loaded model from {model_path}\")\n",
    "        \n",
    "        # Load scaler\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        print(f\"‚úì Loaded scaler from {scaler_path}\")\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"\n",
    "        Predict emotion from image file\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to image file\n",
    "            \n",
    "        Returns:\n",
    "            dict with prediction results\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.feature_extractor.extract_features(image)\n",
    "        \n",
    "        if features is None:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Could not extract features (no face detected)',\n",
    "                'image': image\n",
    "            }\n",
    "        \n",
    "        # Standardize features\n",
    "        features_scaled = self.scaler.transform(features.reshape(1, -1))\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(features_scaled)[0]\n",
    "        \n",
    "        # Get prediction probabilities if available\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            probabilities = self.model.predict_proba(features_scaled)[0]\n",
    "        elif hasattr(self.model, 'decision_function'):\n",
    "            # For SVM, convert decision function to pseudo-probabilities\n",
    "            decision = self.model.decision_function(features_scaled)[0]\n",
    "            # Softmax normalization\n",
    "            exp_decision = np.exp(decision - np.max(decision))\n",
    "            probabilities = exp_decision / exp_decision.sum()\n",
    "        else:\n",
    "            probabilities = None\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'emotion': self.emotions[prediction],\n",
    "            'emotion_idx': prediction,\n",
    "            'probabilities': probabilities,\n",
    "            'image': image,\n",
    "            'features': features\n",
    "        }\n",
    "    \n",
    "    def predict_and_visualize(self, image_path):\n",
    "        \"\"\"Predict and visualize result\"\"\"\n",
    "        result = self.predict(image_path)\n",
    "        \n",
    "        if not result['success']:\n",
    "            print(f\"‚ùå Prediction failed: {result['error']}\")\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(result['image'], cmap='gray')\n",
    "            plt.title(f\"Error: {result['error']}\", color='red')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            return result\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Image with prediction\n",
    "        axes[0].imshow(result['image'], cmap='gray')\n",
    "        axes[0].set_title(f\"Predicted: {result['emotion']}\", \n",
    "                         fontsize=14, fontweight='bold', color='green')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Probability bar chart\n",
    "        if result['probabilities'] is not None:\n",
    "            axes[1].barh(self.emotions, result['probabilities'], color='skyblue')\n",
    "            axes[1].axvline(x=result['probabilities'][result['emotion_idx']], \n",
    "                           color='green', linestyle='--', linewidth=2)\n",
    "            axes[1].set_xlabel('Probability', fontsize=12)\n",
    "            axes[1].set_title('Emotion Probabilities', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlim([0, 1])\n",
    "            \n",
    "            # Highlight predicted class\n",
    "            for i, (emotion, prob) in enumerate(zip(self.emotions, result['probabilities'])):\n",
    "                if i == result['emotion_idx']:\n",
    "                    axes[1].get_yticklabels()[i].set_weight('bold')\n",
    "                    axes[1].get_yticklabels()[i].set_color('green')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'Probabilities not available', \n",
    "                        ha='center', va='center', fontsize=12)\n",
    "            axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PREDICTION RESULT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Image: {image_path}\")\n",
    "        print(f\"Predicted Emotion: {result['emotion']}\")\n",
    "        if result['probabilities'] is not None:\n",
    "            print(f\"Confidence: {result['probabilities'][result['emotion_idx']]:.4f}\")\n",
    "            print(f\"\\nAll probabilities:\")\n",
    "            for emotion, prob in zip(self.emotions, result['probabilities']):\n",
    "                print(f\"  {emotion:10s}: {prob:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize predictor with best model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING EMOTION PREDICTOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = max(results.keys(), key=lambda k: results[k]['test_acc'])\n",
    "print(f\"\\nUsing best model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results[best_model_name]['test_acc']:.4f}\")\n",
    "\n",
    "# Set paths based on model type\n",
    "if results[best_model_name]['feature_type'] == 'CNN':\n",
    "    model_path = 'model_svm_cnn.pkl' if 'SVM' in best_model_name else 'model_logreg_cnn.pkl'\n",
    "    scaler_path = 'scaler_cnn.pkl'\n",
    "    feature_extractor = cnn_extractor\n",
    "    feature_type = 'CNN'\n",
    "else:\n",
    "    model_path = 'model_svm_landmarks.pkl' if 'SVM' in best_model_name else 'model_rf_landmarks.pkl'\n",
    "    scaler_path = 'scaler_landmarks.pkl'\n",
    "    feature_extractor = landmark_extractor\n",
    "    feature_type = 'Landmarks'\n",
    "\n",
    "# Create predictor\n",
    "predictor = EmotionPredictor(\n",
    "    model_path=model_path,\n",
    "    scaler_path=scaler_path,\n",
    "    feature_extractor=feature_extractor,\n",
    "    emotions=EMOTIONS,\n",
    "    feature_type=feature_type\n",
    ")\n",
    "\n",
    "print(f\"‚úì Predictor ready (Feature type: {feature_type})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# Demonstrate on unseen images\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING ON UNSEEN IMAGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select random images from test set for demonstration\n",
    "def get_random_test_images(data_dir, emotions, num_images=2):\n",
    "    \"\"\"Get random test images\"\"\"\n",
    "    selected_images = []\n",
    "    \n",
    "    for emotion in np.random.choice(emotions, num_images, replace=False):\n",
    "        emotion_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(emotion_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(emotion_path) \n",
    "                 if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "        if len(images) > 0:\n",
    "            img_name = np.random.choice(images)\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            selected_images.append((img_path, emotion))\n",
    "    \n",
    "    return selected_images\n",
    "\n",
    "# Get test images\n",
    "test_images = get_random_test_images(TEST_DIR, EMOTIONS, num_images=3)\n",
    "\n",
    "# Predict on each image\n",
    "for i, (img_path, true_emotion) in enumerate(test_images, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST IMAGE {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"True emotion: {true_emotion}\")\n",
    "    \n",
    "    result = predictor.predict_and_visualize(img_path)\n",
    "\n",
    "# ============================================================================\n",
    "# Save predictor for future use\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING PREDICTOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open('emotion_predictor.pkl', 'wb') as f:\n",
    "    pickle.dump(predictor, f)\n",
    "\n",
    "print(\"‚úì Predictor saved to 'emotion_predictor.pkl'\")\n",
    "print(\"\\nTo use in the future:\")\n",
    "print(\"  with open('emotion_predictor.pkl', 'rb') as f:\")\n",
    "print(\"      predictor = pickle.load(f)\")\n",
    "print(\"  result = predictor.predict('path/to/image.jpg')\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eff9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Load Previously Extracted Features\n",
    "# Use this if you've already extracted features and want to skip that step\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_saved_features(landmarks=True, cnn=True):\n",
    "    \"\"\"\n",
    "    Load previously saved features\n",
    "    \n",
    "    Args:\n",
    "        landmarks: Load landmark features\n",
    "        cnn: Load CNN features\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with loaded features\n",
    "    \"\"\"\n",
    "    loaded = {}\n",
    "    \n",
    "    if landmarks:\n",
    "        try:\n",
    "            print(\"Loading landmark features...\")\n",
    "            train_data = np.load('features_train_landmarks.npz')\n",
    "            test_data = np.load('features_test_landmarks.npz')\n",
    "            \n",
    "            loaded['X_train_landmarks'] = train_data['X']\n",
    "            loaded['y_train'] = train_data['y']\n",
    "            loaded['X_test_landmarks'] = test_data['X']\n",
    "            loaded['y_test'] = test_data['y']\n",
    "            loaded['emotions'] = train_data['emotions']\n",
    "            \n",
    "            print(f\"‚úì Landmark features loaded\")\n",
    "            print(f\"  Train: {loaded['X_train_landmarks'].shape}\")\n",
    "            print(f\"  Test:  {loaded['X_test_landmarks'].shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå Landmark feature files not found\")\n",
    "    \n",
    "    if cnn:\n",
    "        try:\n",
    "            print(\"\\nLoading CNN features...\")\n",
    "            train_data = np.load('features_train_cnn.npz')\n",
    "            test_data = np.load('features_test_cnn.npz')\n",
    "            \n",
    "            loaded['X_train_cnn'] = train_data['X']\n",
    "            loaded['y_train_cnn'] = train_data['y']\n",
    "            loaded['X_test_cnn'] = test_data['X']\n",
    "            loaded['y_test_cnn'] = test_data['y']\n",
    "            \n",
    "            print(f\"‚úì CNN features loaded\")\n",
    "            print(f\"  Train: {loaded['X_train_cnn'].shape}\")\n",
    "            print(f\"  Test:  {loaded['X_test_cnn'].shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå CNN feature files not found\")\n",
    "    \n",
    "    return loaded\n",
    "\n",
    "# Example usage:\n",
    "# features = load_saved_features(landmarks=True, cnn=True)\n",
    "# X_train_landmarks = features['X_train_landmarks']\n",
    "# y_train = features['y_train']\n",
    "# etc.\n",
    "\n",
    "def load_saved_models():\n",
    "    \"\"\"Load all saved models and scalers\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    loaded_models = {}\n",
    "    \n",
    "    model_files = {\n",
    "        'SVM_Landmarks': ('model_svm_landmarks.pkl', 'scaler_landmarks.pkl'),\n",
    "        'RF_Landmarks': ('model_rf_landmarks.pkl', None),\n",
    "        'SVM_CNN': ('model_svm_cnn.pkl', 'scaler_cnn.pkl'),\n",
    "        'LogReg_CNN': ('model_logreg_cnn.pkl', 'scaler_cnn.pkl')\n",
    "    }\n",
    "    \n",
    "    for model_name, (model_path, scaler_path) in model_files.items():\n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            scaler = None\n",
    "            if scaler_path:\n",
    "                with open(scaler_path, 'rb') as f:\n",
    "                    scaler = pickle.load(f)\n",
    "            \n",
    "            loaded_models[model_name] = {\n",
    "                'model': model,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "            print(f\"‚úì Loaded {model_name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Could not load {model_name}\")\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "# Example usage:\n",
    "# models = load_saved_models()\n",
    "# svm_model = models['SVM_CNN']['model']\n",
    "# svm_scaler = models['SVM_CNN']['scaler']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
