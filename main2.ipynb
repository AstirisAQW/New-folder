{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951eaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import mediapipe as mp\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Define paths and classes\n",
    "BASE_DIR = \"FER-2013\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test\")\n",
    "CLASSES = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "print(\"Libraries imported and configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads images (48x48 grayscale, converted to 3-channel BGR) and labels.\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_map = {emotion: i for i, emotion in enumerate(CLASSES)}\n",
    "    \n",
    "    print(f\"Loading data from: {data_dir}\")\n",
    "    \n",
    "    for emotion in CLASSES:\n",
    "        class_path = os.path.join(data_dir, emotion)\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"Warning: Path not found: {class_path}\")\n",
    "            continue\n",
    "            \n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.png')):\n",
    "                img_path = os.path.join(class_path, filename)\n",
    "                # Read image as grayscale\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if img is not None and img.shape == (48, 48):\n",
    "                    # Convert grayscale to 3 channels (required for MediaPipe/CNN backbones)\n",
    "                    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "                    data.append(img_bgr)\n",
    "                    labels.append(label_map[emotion])\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "X_train_raw, y_train = load_data(TRAIN_DIR)\n",
    "print(\"Loading test data...\")\n",
    "X_test_raw, y_test = load_data(TEST_DIR)\n",
    "\n",
    "print(\"\\n--- Train–Test split overview ---\")\n",
    "print(f\"Train set shape: {X_train_raw.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test_raw.shape}, Labels shape: {y_test.shape}\")\n",
    "\n",
    "# Class distribution check\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "print(\"\\nClass Counts (Train):\", {CLASSES[i]: count for i, count in train_counts.items()})\n",
    "print(\"Class Counts (Test):\", {CLASSES[i]: count for i, count in test_counts.items()})\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"\\nClass Weights for handling imbalance:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7228cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize MediaPipe components for Landmark extraction\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Indices for key points (used for normalization in Track A)\n",
    "    LEFT_EYE_IDX = 133\n",
    "    RIGHT_EYE_IDX = 362\n",
    "    \n",
    "    # Test MediaPipe initialization\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3) as test_face_mesh:\n",
    "        test_result = test_face_mesh.process(np.ones((48, 48, 3), dtype=np.uint8))\n",
    "    print(\"MediaPipe initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"MediaPipe initialization failed: {e}\")\n",
    "    print(\"Falling back to Haar Cascade for face detection\")\n",
    "    \n",
    "    # Initialize Haar Cascade as fallback\n",
    "    haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    mp_face_mesh = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 1 - Face Detection & Visual Check\n",
    "def detect_faces_mediapipe(image_bgr, face_mesh):\n",
    "    \"\"\"Detects faces using MediaPipe Face Mesh.\"\"\"\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    display_image = image_bgr.copy()\n",
    "    face_box = None\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        h, w, c = image_bgr.shape\n",
    "        \n",
    "        # Calculate bounding box from landmarks\n",
    "        x_min, y_min = w, h\n",
    "        x_max, y_max = 0, 0\n",
    "        for landmark in landmarks.landmark:\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "            x_min = min(x_min, x)\n",
    "            y_min = min(y_min, y)\n",
    "            x_max = max(x_max, x)\n",
    "            y_max = max(y_max, y)\n",
    "            \n",
    "        # Draw bounding box\n",
    "        padding = 2\n",
    "        x_min = max(0, x_min - padding)\n",
    "        y_min = max(0, y_min - padding)\n",
    "        x_max = min(w, x_max + padding)\n",
    "        y_max = min(h, y_max + padding)\n",
    "        \n",
    "        cv2.rectangle(display_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 1)\n",
    "        face_box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "        \n",
    "    return cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB), face_box\n",
    "\n",
    "def detect_faces_haar(image_bgr, cascade):\n",
    "    \"\"\"Detects faces using Haar Cascade as fallback.\"\"\"\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    faces = cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    display_image = image_bgr.copy()\n",
    "    face_box = None\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        # Use the largest face\n",
    "        faces = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)\n",
    "        x, y, w, h = faces[0]\n",
    "        cv2.rectangle(display_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        face_box = (x, y, w, h)\n",
    "    \n",
    "    return cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB), face_box\n",
    "\n",
    "def visualize_face_detection_samples(X_raw, y_raw, num_samples=8):\n",
    "    \"\"\"Visualize face detection results on samples.\"\"\"\n",
    "    label_names = {i: emotion for i, emotion in enumerate(CLASSES)}\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Get diverse samples from different classes\n",
    "    sample_indices = []\n",
    "    for class_idx in range(len(CLASSES)):\n",
    "        class_indices = np.where(y_raw == class_idx)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            sample_indices.append(class_indices[0])\n",
    "        if len(sample_indices) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    detection_count = 0\n",
    "    \n",
    "    if mp_face_mesh is not None:\n",
    "        # Use MediaPipe\n",
    "        with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3) as face_mesh:\n",
    "            for idx, sample_idx in enumerate(sample_indices):\n",
    "                if idx >= len(axes): \n",
    "                    break\n",
    "                    \n",
    "                image_bgr = X_raw[sample_idx]\n",
    "                emotion = label_names[y_raw[sample_idx]]\n",
    "                \n",
    "                display_image_rgb, face_box = detect_faces_mediapipe(image_bgr, face_mesh)\n",
    "                if face_box:\n",
    "                    detection_count += 1\n",
    "                \n",
    "                axes[idx].imshow(display_image_rgb)\n",
    "                detection_status = \"Detected\" if face_box else \"Not Detected\"\n",
    "                axes[idx].set_title(f'{emotion} - {detection_status}')\n",
    "                axes[idx].axis('off')\n",
    "    else:\n",
    "        # Use Haar Cascade\n",
    "        for idx, sample_idx in enumerate(sample_indices):\n",
    "            if idx >= len(axes): \n",
    "                break\n",
    "                \n",
    "            image_bgr = X_raw[sample_idx]\n",
    "            emotion = label_names[y_raw[sample_idx]]\n",
    "            \n",
    "            display_image_rgb, face_box = detect_faces_haar(image_bgr, haar_cascade)\n",
    "            if face_box:\n",
    "                detection_count += 1\n",
    "            \n",
    "            axes[idx].imshow(display_image_rgb)\n",
    "            detection_status = \"Detected\" if face_box else \"Not Detected\"\n",
    "            axes[idx].set_title(f'{emotion} - {detection_status}')\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    for idx in range(len(sample_indices), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    detection_rate = detection_count / len(sample_indices)\n",
    "    plt.suptitle(f\"Milestone 1: Face Detection Check (Detection Rate: {detection_rate:.1%})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return detection_rate\n",
    "\n",
    "print(\"Running Milestone 1: Face Detection Visualization...\")\n",
    "detection_rate = visualize_face_detection_samples(X_train_raw, y_train)\n",
    "print(f\"Face detection rate on samples: {detection_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Milestone 2 - Track A: Landmark Feature Extraction\n",
    "def extract_and_normalize_landmarks(image_bgr, face_mesh):\n",
    "    \"\"\"Extracts 468 normalized landmarks (centered by eye midpoint, scaled by IPD).\"\"\"\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "    \n",
    "    if not results.multi_face_landmarks:\n",
    "        return None\n",
    "    \n",
    "    landmarks = results.multi_face_landmarks[0].landmark\n",
    "    \n",
    "    coords = np.array([[lm.x, lm.y] for lm in landmarks])\n",
    "    \n",
    "    # Calculate IPD and center\n",
    "    p_left = coords[LEFT_EYE_IDX]\n",
    "    p_right = coords[RIGHT_EYE_IDX]\n",
    "    ipd = np.linalg.norm(p_left - p_right)\n",
    "    \n",
    "    if ipd == 0: \n",
    "        return None\n",
    "        \n",
    "    center = (p_left + p_right) / 2\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_coords = (coords - center) / ipd\n",
    "    \n",
    "    return normalized_coords.flatten() # 936 features\n",
    "\n",
    "def get_landmark_features_and_labels(X_raw, y_raw):\n",
    "    \"\"\"Extract landmark features with progress tracking.\"\"\"\n",
    "    features = []\n",
    "    filtered_labels = []\n",
    "    failed_detections = 0\n",
    "    \n",
    "    print(\"Extracting landmark features...\")\n",
    "    \n",
    "    if mp_face_mesh is None:\n",
    "        print(\"MediaPipe not available. Cannot extract landmarks.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3) as face_mesh:\n",
    "        for i, img in enumerate(X_raw):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Processed {i}/{len(X_raw)} images...\")\n",
    "                \n",
    "            feature = extract_and_normalize_landmarks(img, face_mesh)\n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "                filtered_labels.append(y_raw[i])\n",
    "            else:\n",
    "                failed_detections += 1\n",
    "                \n",
    "    print(f\"Landmark extraction complete. Failed detections: {failed_detections}/{len(X_raw)} ({failed_detections/len(X_raw):.1%})\")\n",
    "    return np.array(features), np.array(filtered_labels)\n",
    "\n",
    "print(\"\\n--- Track A: Landmark Feature Extraction (936 features) ---\")\n",
    "X_train_lm, y_train_lm = get_landmark_features_and_labels(X_train_raw, y_train)\n",
    "X_test_lm, y_test_lm = get_landmark_features_and_labels(X_test_raw, y_test)\n",
    "\n",
    "if len(X_train_lm) > 0:\n",
    "    print(f\"Landmark Train Features shape: {X_train_lm.shape}, Labels shape: {y_train_lm.shape}\")\n",
    "    print(f\"Landmark Test Features shape: {X_test_lm.shape}, Labels shape: {y_test_lm.shape}\")\n",
    "else:\n",
    "    print(\"No landmark features extracted. Proceeding with CNN features only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Milestone 2 - Track B: CNN Deep Feature Extraction\n",
    "def extract_cnn_features_batched(X_raw, batch_size=32):\n",
    "    \"\"\"Extracts deep features using MobileNetV2 in batches to manage memory.\"\"\"\n",
    "    \n",
    "    num_images = len(X_raw)\n",
    "    all_features = []\n",
    "    \n",
    "    print(f\"Starting CNN feature extraction for {num_images} images with batch size {batch_size}...\")\n",
    "    \n",
    "    for i in range(0, num_images, batch_size):\n",
    "        batch_raw = X_raw[i:i + batch_size]\n",
    "        \n",
    "        # 1. Resize batch\n",
    "        resized_images = [cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA) for img in batch_raw]\n",
    "        X_resized_batch = np.array(resized_images)\n",
    "        \n",
    "        # 2. Preprocess batch\n",
    "        X_processed_batch = preprocess_input(X_resized_batch)\n",
    "        \n",
    "        # 3. Extract features\n",
    "        features_batch = feature_extractor.predict(X_processed_batch, verbose=0)\n",
    "        all_features.append(features_batch)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del X_resized_batch, X_processed_batch, features_batch\n",
    "        gc.collect()\n",
    "        \n",
    "        if (i + batch_size) % (batch_size * 10) == 0:\n",
    "            print(f\"Processed {i + batch_size}/{num_images} images.\")\n",
    "            \n",
    "    return np.concatenate(all_features, axis=0)\n",
    "\n",
    "# Load MobileNetV2 backbone (1280 features)\n",
    "print(\"Loading MobileNetV2 feature extractor...\")\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "print(\"\\n--- Track B: CNN Deep Feature Extraction (1280 features) ---\")\n",
    "X_train_cnn = extract_cnn_features_batched(X_train_raw)\n",
    "X_test_cnn = extract_cnn_features_batched(X_test_raw)\n",
    "\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn = y_test\n",
    "\n",
    "print(f\"CNN Train Features shape: {X_train_cnn.shape}, Labels shape: {y_train_cnn.shape}\")\n",
    "print(f\"CNN Test Features shape: {X_test_cnn.shape}, Labels shape: {y_test_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bed468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Training with Improved Parameters\n",
    "def train_and_evaluate_models():\n",
    "    \"\"\"Train and evaluate both models with improved parameters.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Model 1: SVM on Landmark Features (if available)\n",
    "    if len(X_train_lm) > 0 and len(X_test_lm) > 0:\n",
    "        print(\"\\n--- Training Model 1: SVM on Landmark Features ---\")\n",
    "        \n",
    "        # Standardization\n",
    "        scaler_lm = StandardScaler()\n",
    "        X_train_lm_scaled = scaler_lm.fit_transform(X_train_lm)\n",
    "        X_test_lm_scaled = scaler_lm.transform(X_test_lm)\n",
    "\n",
    "        # Training SVM with class weights\n",
    "        svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=RANDOM_SEED, class_weight='balanced')\n",
    "        svm_model.fit(X_train_lm_scaled, y_train_lm)\n",
    "        y_pred_svm = svm_model.predict(X_test_lm_scaled)\n",
    "\n",
    "        print(\"SVM Training Complete. (C=1.0, RBF kernel, class_weight='balanced')\")\n",
    "        results['svm'] = (svm_model, y_pred_svm, y_test_lm, \"SVM (Landmarks)\")\n",
    "    \n",
    "    # Model 2: Random Forest on CNN Features\n",
    "    print(\"\\n--- Training Model 2: Random Forest on CNN Features ---\")\n",
    "    \n",
    "    # Standardize CNN features for better performance\n",
    "    scaler_cnn = StandardScaler()\n",
    "    X_train_cnn_scaled = scaler_cnn.fit_transform(X_train_cnn)\n",
    "    X_test_cnn_scaled = scaler_cnn.transform(X_test_cnn)\n",
    "    \n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=15, \n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=RANDOM_SEED, \n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    rf_model.fit(X_train_cnn_scaled, y_train_cnn)\n",
    "    y_pred_rf = rf_model.predict(X_test_cnn_scaled)\n",
    "\n",
    "    print(\"Random Forest Training Complete. (200 estimators, max_depth=15, balanced)\")\n",
    "    results['rf'] = (rf_model, y_pred_rf, y_test_cnn, \"Random Forest (CNN)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train models\n",
    "model_results = train_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b689af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Milestone 3 - Comprehensive Evaluation\n",
    "def evaluate_model(y_true, y_pred, model_name, feature_type):\n",
    "    \"\"\"Prints evaluation metrics and plots confusion matrix.\"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\n--- Evaluation: {model_name} ({feature_type}) ---\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred, target_names=CLASSES, output_dict=True)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.title(f'Confusion Matrix: {model_name} ({feature_type})')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze common confusions\n",
    "    print(\"\\nKey Confusion Patterns:\")\n",
    "    for i in range(len(CLASSES)):\n",
    "        for j in range(len(CLASSES)):\n",
    "            if i != j and cm[i, j] > 0.1 * cm[i, i]:  # Significant confusion\n",
    "                print(f\"  {CLASSES[i]} → {CLASSES[j]}: {cm[i, j]} instances\")\n",
    "    \n",
    "    return accuracy, macro_f1, report, cm\n",
    "\n",
    "# Evaluate all trained models\n",
    "evaluation_results = {}\n",
    "for key, (model, y_pred, y_true, model_name) in model_results.items():\n",
    "    feature_type = \"Landmarks (Track A)\" if \"Landmark\" in model_name else \"CNN Embeddings (Track B)\"\n",
    "    acc, f1, report, cm = evaluate_model(y_true, y_pred, model_name, feature_type)\n",
    "    evaluation_results[key] = (acc, f1, report, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33390265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Comparative Summary and Insights\n",
    "def create_comparative_summary(evaluation_results):\n",
    "    \"\"\"Create comprehensive comparison of models.\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for key, (acc, f1, report, cm) in evaluation_results.items():\n",
    "        model_name = \"SVM (Landmarks)\" if key == 'svm' else \"Random Forest (CNN Embeddings)\"\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'Macro Precision': report['macro avg']['precision'],\n",
    "            'Macro Recall': report['macro avg']['recall'],\n",
    "            'Macro F1': f1\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data).set_index('Model')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARATIVE SUMMARY TABLE\")\n",
    "    print(\"=\"*60)\n",
    "    print(summary_df.round(4))\n",
    "    \n",
    "    # Determine best model\n",
    "    best_model_idx = summary_df['Accuracy'].idxmax()\n",
    "    best_accuracy = summary_df.loc[best_model_idx, 'Accuracy']\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_idx} (Accuracy: {best_accuracy:.2%})\")\n",
    "    \n",
    "    # Insights\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY INSIGHTS & OBSERVATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    insights = [\n",
    "        \"• Class imbalance significantly affects performance, especially for 'disgust' class\",\n",
    "        \"• Fear and surprise are commonly confused due to similar facial expressions (wide eyes)\",\n",
    "        \"• Happy emotions are generally well-classified across both models\",\n",
    "        \"• Geometric features (landmarks) may struggle with subtle expression differences\",\n",
    "        \"• CNN features capture more complex patterns but require careful preprocessing\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(insight)\n",
    "    \n",
    "    return summary_df, best_model_idx\n",
    "\n",
    "summary_df, best_model_name = create_comparative_summary(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Simple Deployment with Best Model\n",
    "# Select the best model for deployment\n",
    "if 'rf' in model_results and 'svm' in model_results:\n",
    "    # Compare which model performed better\n",
    "    rf_acc = evaluation_results['rf'][0]\n",
    "    svm_acc = evaluation_results['svm'][0]\n",
    "    \n",
    "    if rf_acc > svm_acc:\n",
    "        BEST_MODEL = model_results['rf'][0]\n",
    "        BEST_FEATURE_TYPE = 'cnn'\n",
    "        print(\"Selected Random Forest (CNN) as best model for deployment\")\n",
    "    else:\n",
    "        BEST_MODEL = model_results['svm'][0]\n",
    "        BEST_FEATURE_TYPE = 'landmarks'\n",
    "        print(\"Selected SVM (Landmarks) as best model for deployment\")\n",
    "elif 'rf' in model_results:\n",
    "    BEST_MODEL = model_results['rf'][0]\n",
    "    BEST_FEATURE_TYPE = 'cnn'\n",
    "    print(\"Selected Random Forest (CNN) as best model for deployment\")\n",
    "else:\n",
    "    BEST_MODEL = None\n",
    "    BEST_FEATURE_TYPE = None\n",
    "    print(\"No suitable model found for deployment\")\n",
    "\n",
    "# Deployment function\n",
    "def predict_emotion(image_path, model, feature_type='cnn'):\n",
    "    \"\"\"\n",
    "    Reads a FER-style face image, runs the pipeline, and predicts the emotion.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return \"Error: No model available for prediction.\"\n",
    "    \n",
    "    # 1. Load Image\n",
    "    img_gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_gray is None:\n",
    "        return \"Error: Could not load image.\"\n",
    "        \n",
    "    img_bgr = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    if feature_type == 'cnn':\n",
    "        # CNN pipeline\n",
    "        img_resized = cv2.resize(img_bgr, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "        X_input = np.expand_dims(img_resized, axis=0)\n",
    "        X_processed = preprocess_input(X_input)\n",
    "        features = feature_extractor.predict(X_processed, verbose=0)\n",
    "        \n",
    "        # Standardize features if using RF with CNN\n",
    "        if hasattr(model, 'estimators_'):  # Random Forest\n",
    "            features = scaler_cnn.transform(features)\n",
    "            \n",
    "    else:\n",
    "        # Landmark pipeline\n",
    "        if mp_face_mesh is None:\n",
    "            return \"Error: MediaPipe not available for landmark extraction.\"\n",
    "            \n",
    "        with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3) as face_mesh:\n",
    "            features = extract_and_normalize_landmarks(img_bgr, face_mesh)\n",
    "            if features is None:\n",
    "                return \"Error: No face detected in image.\"\n",
    "            features = scaler_lm.transform([features])\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = model.predict(features)\n",
    "    predicted_label_index = prediction[0]\n",
    "    \n",
    "    return CLASSES[predicted_label_index]\n",
    "\n",
    "# Test deployment on sample images\n",
    "def test_deployment(num_samples=3):\n",
    "    \"\"\"Test the deployment function on random test samples.\"\"\"\n",
    "    print(f\"\\n--- Testing Deployment on {num_samples} Random Samples ---\")\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Randomly select a test image\n",
    "        random_idx = np.random.randint(len(X_test_raw))\n",
    "        test_image = X_test_raw[random_idx]\n",
    "        true_emotion = CLASSES[y_test[random_idx]]\n",
    "        \n",
    "        # Save temporary image for prediction\n",
    "        temp_path = f\"temp_test_{i}.jpg\"\n",
    "        cv2.imwrite(temp_path, test_image)\n",
    "        \n",
    "        # Predict\n",
    "        predicted_emotion = predict_emotion(temp_path, BEST_MODEL, BEST_FEATURE_TYPE)\n",
    "        \n",
    "        print(f\"Sample {i+1}: True='{true_emotion}', Predicted='{predicted_emotion}' {'✅' if true_emotion == predicted_emotion else '❌'}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "if BEST_MODEL is not None:\n",
    "    test_deployment()\n",
    "else:\n",
    "    print(\"Skipping deployment test - no model available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6071523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Final Summary and Cleanup\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"   Training samples: {len(X_train_raw):,}\")\n",
    "print(f\"   Test samples: {len(X_test_raw):,}\")\n",
    "print(f\"   Classes: {CLASSES}\")\n",
    "\n",
    "print(f\"\\nTechnical Implementation:\")\n",
    "print(f\"   Face detection: {'MediaPipe' if mp_face_mesh else 'Haar Cascade'}\")\n",
    "print(f\"   Feature tracks: {'Landmarks + CNN' if len(X_train_lm) > 0 else 'CNN only'}\")\n",
    "print(f\"   Models trained: {len(model_results)}\")\n",
    "\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "for key, (acc, f1, report, cm) in evaluation_results.items():\n",
    "    model_name = \"SVM (Landmarks)\" if key == 'svm' else \"Random Forest (CNN)\"\n",
    "    print(f\"   {model_name}: Accuracy = {acc:.2%}, F1 = {f1:.2%}\")\n",
    "\n",
    "print(f\"\\nDeployment Ready: {BEST_MODEL is not None}\")\n",
    "if BEST_MODEL is not None:\n",
    "    best_model_type = \"Random Forest (CNN)\" if BEST_FEATURE_TYPE == 'cnn' else \"SVM (Landmarks)\"\n",
    "    print(f\"   Best model: {best_model_type}\")\n",
    "\n",
    "print(f\"\\nRecommendations for Improvement:\")\n",
    "improvements = [\n",
    "    \"1. Address class imbalance with data augmentation\",\n",
    "    \"2. Try ensemble methods combining both feature types\",\n",
    "    \"3. Experiment with different CNN architectures\",\n",
    "    \"4. Add more sophisticated face detection fallbacks\",\n",
    "    \"5. Implement cross-validation for hyperparameter tuning\"\n",
    "]\n",
    "\n",
    "for imp in improvements:\n",
    "    print(f\"   {imp}\")\n",
    "\n",
    "# Cleanup\n",
    "gc.collect()\n",
    "print(\"\\nCode execution completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
