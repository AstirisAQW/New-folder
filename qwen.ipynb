{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ce496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths and classes\n",
    "DATA_DIR = \"FER-2013\"\n",
    "CLASSES = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "IMG_SIZE = (48, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93acf3",
   "metadata": {},
   "source": [
    "Milestone 1 — Face Detection & Visual Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Verify dataset structure and display sample images\n",
    "def verify_and_show_samples(data_type=\"train\", samples_per_class=2):\n",
    "    base_path = os.path.join(DATA_DIR, data_type)\n",
    "    fig, axes = plt.subplots(len(CLASSES), samples_per_class, figsize=(6, 15))\n",
    "    for i, emotion in enumerate(CLASSES):\n",
    "        class_path = os.path.join(base_path, emotion)\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"⚠️ Missing folder: {class_path}\")\n",
    "            continue\n",
    "        files = os.listdir(class_path)\n",
    "        print(f\"{emotion}: {len(files)} images\")\n",
    "        for j in range(min(samples_per_class, len(files))):\n",
    "            img_path = os.path.join(class_path, files[j])\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].set_title(emotion)\n",
    "            axes[i, j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "verify_and_show_samples(\"train\", 2)\n",
    "verify_and_show_samples(\"test\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: MediaPipe Face Detection + Bounding Box\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_and_draw_face(image, min_detection_confidence=0.5):\n",
    "    \"\"\"Takes grayscale 48x48 image, converts to RGB, detects face, draws box.\"\"\"\n",
    "    rgb_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=min_detection_confidence\n",
    "    ) as face_detector:\n",
    "        results = face_detector.process(rgb_img)\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                mp_drawing.draw_detection(rgb_img, detection)\n",
    "            return rgb_img\n",
    "        else:\n",
    "            return rgb_img  # return original if no face\n",
    "\n",
    "# Test on a few samples\n",
    "def test_face_detection():\n",
    "    base_path = os.path.join(DATA_DIR, \"train\")\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 6))\n",
    "    idx = 0\n",
    "    for emotion in CLASSES:\n",
    "        class_path = os.path.join(base_path, emotion)\n",
    "        files = os.listdir(class_path)\n",
    "        if files:\n",
    "            img_path = os.path.join(class_path, files[0])\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            detected = detect_and_draw_face(img)\n",
    "            ax = axes[idx // 4, idx % 4]\n",
    "            ax.imshow(detected)\n",
    "            ax.set_title(emotion)\n",
    "            ax.axis('off')\n",
    "            idx += 1\n",
    "            if idx >= 8:\n",
    "                break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_face_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6799db",
   "metadata": {},
   "source": [
    "Milestone 2 — Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e20d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Image loading utility\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load {path}\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e5a51",
   "metadata": {},
   "source": [
    "Track A — Landmark Features (MediaPipe FaceMesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a067d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Landmark-based feature extraction\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "def extract_landmark_features(image):\n",
    "    \"\"\"Returns normalized 468x2 landmark array flattened to 936-dim vector, or zeros if no face.\"\"\"\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        results = face_mesh.process(rgb)\n",
    "        if not results.multi_face_landmarks:\n",
    "            return np.zeros(936)  # fallback\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        h, w = image.shape\n",
    "        coords = np.array([[lm.x * w, lm.y * h] for lm in landmarks])  # (468, 2)\n",
    "\n",
    "        # Normalize: center on nose tip (landmark 1) and scale by inter-pupillary distance\n",
    "        left_eye = coords[33]   # left eye outer\n",
    "        right_eye = coords[263] # right eye outer\n",
    "        ipd = np.linalg.norm(left_eye - right_eye)\n",
    "        if ipd == 0:\n",
    "            ipd = 1.0\n",
    "        nose = coords[1]\n",
    "        coords = (coords - nose) / ipd\n",
    "        return coords.flatten()\n",
    "\n",
    "def prepare_landmark_dataset(data_type=\"train\"):\n",
    "    features, labels = [], []\n",
    "    base_path = os.path.join(DATA_DIR, data_type)\n",
    "    for label_idx, emotion in enumerate(CLASSES):\n",
    "        class_path = os.path.join(base_path, emotion)\n",
    "        files = os.listdir(class_path)\n",
    "        for file in files[:500]:  # limit for speed; adjust as needed\n",
    "            try:\n",
    "                img = load_image(os.path.join(class_path, file))\n",
    "                feat = extract_landmark_features(img)\n",
    "                features.append(feat)\n",
    "                labels.append(label_idx)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "print(\"Extracting landmark features (may take a few minutes)...\")\n",
    "X_train_land, y_train_land = prepare_landmark_dataset(\"train\")\n",
    "X_test_land, y_test_land = prepare_landmark_dataset(\"test\")\n",
    "print(\"Landmark shapes:\", X_train_land.shape, X_test_land.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e32b31",
   "metadata": {},
   "source": [
    "Track B — CNN Deep Features (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40bc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: CNN feature extraction using MobileNetV2\n",
    "# Build feature extractor (remove top layers)\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "def preprocess_for_cnn(image):\n",
    "    \"\"\"Resize 48x48 grayscale → 224x224 RGB, normalize.\"\"\"\n",
    "    resized = cv2.resize(image, (224, 224))\n",
    "    rgb = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)\n",
    "    rgb = rgb.astype(np.float32) / 255.0\n",
    "    return np.expand_dims(rgb, axis=0)\n",
    "\n",
    "def extract_cnn_features(image):\n",
    "    preprocessed = preprocess_for_cnn(image)\n",
    "    features = feature_extractor(preprocessed)\n",
    "    return tf.reduce_mean(features, axis=[1, 2]).numpy().flatten()  # global avg pool → (1280,)\n",
    "\n",
    "def prepare_cnn_dataset(data_type=\"train\"):\n",
    "    features, labels = [], []\n",
    "    base_path = os.path.join(DATA_DIR, data_type)\n",
    "    for label_idx, emotion in enumerate(CLASSES):\n",
    "        class_path = os.path.join(base_path, emotion)\n",
    "        files = os.listdir(class_path)\n",
    "        for file in files[:500]:  # limit for speed\n",
    "            try:\n",
    "                img = load_image(os.path.join(class_path, file))\n",
    "                feat = extract_cnn_features(img)\n",
    "                features.append(feat)\n",
    "                labels.append(label_idx)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "print(\"Extracting CNN features...\")\n",
    "X_train_cnn, y_train_cnn = prepare_cnn_dataset(\"train\")\n",
    "X_test_cnn, y_test_cnn = prepare_cnn_dataset(\"test\")\n",
    "print(\"CNN shapes:\", X_train_cnn.shape, X_test_cnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbaf580",
   "metadata": {},
   "source": [
    "Model Training (Both Tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d774dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Train models\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, track_name=\"Landmark\"):\n",
    "    print(f\"\\n=== Training on {track_name} Features ===\")\n",
    "    \n",
    "    # Standardize for distance-based models\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Model 1: SVM\n",
    "    svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred_svm = svm.predict(X_test_scaled)\n",
    "    \n",
    "    # Model 2: Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)  # RF doesn't need scaling\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    def report(name, y_true, y_pred):\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        print(f\"{name} → Acc: {acc:.3f}, Macro-F1: {f1:.3f}\")\n",
    "        return acc, f1, y_pred\n",
    "    \n",
    "    acc_svm, f1_svm, pred_svm = report(\"SVM\", y_test, y_pred_svm)\n",
    "    acc_rf, f1_rf, pred_rf = report(\"Random Forest\", y_test, y_pred_rf)\n",
    "    \n",
    "    return {\n",
    "        'SVM': (svm, scaler, pred_svm),\n",
    "        'RF': (rf, None, pred_rf)\n",
    "    }, (acc_svm, f1_svm), (acc_rf, f1_rf)\n",
    "\n",
    "# Train both tracks\n",
    "models_land, svm_land_metrics, rf_land_metrics = train_and_evaluate(\n",
    "    X_train_land, y_train_land, X_test_land, y_test_land, \"Landmark\"\n",
    ")\n",
    "\n",
    "models_cnn, svm_cnn_metrics, rf_cnn_metrics = train_and_evaluate(\n",
    "    X_train_cnn, y_train_cnn, X_test_cnn, y_test_cnn, \"CNN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096f7b2",
   "metadata": {},
   "source": [
    "Milestone 3 — Evaluation & Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluation reports and confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(CLASSES))\n",
    "    plt.xticks(tick_marks, CLASSES, rotation=45)\n",
    "    plt.yticks(tick_marks, CLASSES)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "\n",
    "# Example: Best model is CNN + SVM (usually)\n",
    "best_model_name = \"CNN_SVM\"\n",
    "best_model, best_scaler, best_pred = models_cnn['SVM']\n",
    "plot_confusion_matrix(y_test_cnn, best_pred, \"CNN + SVM Confusion Matrix\")\n",
    "\n",
    "# Also show Landmark + RF for comparison\n",
    "_, _, pred_land_rf = models_land['RF']\n",
    "plot_confusion_matrix(y_test_land, pred_land_rf, \"Landmark + RF Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9727e",
   "metadata": {},
   "source": [
    "Simple Deployment: predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Prediction pipeline\n",
    "def predict_emotion(image_path, model=models_cnn['SVM'][0], scaler=models_cnn['SVM'][1], feature_type='cnn'):\n",
    "    \"\"\"\n",
    "    Predict emotion from a single FER-style image (48x48 grayscale).\n",
    "    Returns: predicted label (string)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found\")\n",
    "    \n",
    "    if feature_type == 'cnn':\n",
    "        feat = extract_cnn_features(img)\n",
    "        if scaler:\n",
    "            feat = scaler.transform([feat])\n",
    "        pred = model.predict(feat)[0]\n",
    "    else:  # landmark\n",
    "        feat = extract_landmark_features(img)\n",
    "        pred = model.predict([feat])[0]\n",
    "    \n",
    "    return CLASSES[pred]\n",
    "\n",
    "# Test on unseen images (place 2 images in 'unseen/' folder)\n",
    "unseen_dir = \"unseen\"\n",
    "if os.path.exists(unseen_dir):\n",
    "    for img_file in os.listdir(unseen_dir)[:2]:\n",
    "        path = os.path.join(unseen_dir, img_file)\n",
    "        pred = predict_emotion(path)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"Prediction: {pred} | File: {img_file}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Create 'unseen/' folder with test images for demo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
